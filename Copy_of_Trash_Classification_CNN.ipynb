{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "nHL-1Us43T8l"
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import os\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import keras.utils as image\n",
    "from IPython.utils import io\n",
    "\n",
    "import seaborn as sb\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "import time\n",
    "import importlib\n",
    "import pathlib\n",
    "\n",
    "import requests\n",
    "from functools import partial\n",
    "\n",
    "size_ = (10,8)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2009 files belonging to 6 classes.\n",
      "Using 1608 files for training.\n",
      "Found 2009 files belonging to 6 classes.\n",
      "Using 401 files for validation.\n",
      "Found 518 files belonging to 6 classes.\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OverfittingCallback(tf.keras.callbacks.Callback):\n",
    "  def on_epoch_end(self, epoch, logs={}):\n",
    "    if logs.get('accuracy') > 0.999:\n",
    "      self.model.stop_training = True\n",
    "      print('Trying to prevent overfitting - stopping training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#center crop of incoming image, sizing it down to (224, 224, 3)\n",
    "def center_crop(img):\n",
    "    image_array = img\n",
    "    shape = image_array.shape # H, W, D\n",
    "    left = int((shape[1]-224)/2)\n",
    "    right = left + 224\n",
    "    bottom = int((shape[0]-224)/2)\n",
    "    top = bottom + 224\n",
    "    return image_array[bottom:top, left:right]\n",
    "\n",
    "#random crop of the image within the defined range\n",
    "def random_crop(img):\n",
    "    image_array = img\n",
    "    shape = image_array.shape\n",
    "    left = np.random.randint(0, shape[1]-224)\n",
    "    right = left + 224\n",
    "    bottom = np.random.randint(0, shape[0]-224)\n",
    "    top = bottom + 224\n",
    "    return image_array[bottom:top, left:right]\n",
    "\n",
    "#with some randomness, either center crop or random crop the image, as well as either flip or do not flip the image.\n",
    "def process_img(img, crop=True):\n",
    "    if crop:\n",
    "        if np.random.rand() > 0.5:\n",
    "            cropped = center_crop(img)\n",
    "            if np.random.rand() > 0.5:\n",
    "                return np.flip(cropped, axis=np.random.randint(0, 2))\n",
    "            else:\n",
    "                return cropped\n",
    "        else:\n",
    "            cropped = random_crop(img)\n",
    "            if np.random.rand() > 0.5:\n",
    "                return np.flip(cropped, axis=np.random.randint(0, 2))\n",
    "            else:\n",
    "                return cropped\n",
    "    else:\n",
    "        return img\n",
    "    \n",
    "\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "id": "WHpMx4BE4fVB"
   },
   "outputs": [],
   "source": [
    "# Just placing everything in a class to make analysis easier\n",
    "class TrashClassifer():\n",
    "    \n",
    "    def __init__(self, home_path='/home/sarp/DS5460', main_dir='garbage_classifier', model_name='vgg16', crop=False):\n",
    "        # Define some constants\n",
    "        self.HOME = Path(home_path)\n",
    "        self.MAIN_DIR = self.HOME / main_dir #the main directory of this project to include all the data\n",
    "        \n",
    "        # Data augmentation switch\n",
    "        self.crop = crop\n",
    "        \n",
    "\n",
    "\n",
    "        self.TEST_PATH = self.MAIN_DIR / 'test'\n",
    "        self.TRAIN_PATH = self.MAIN_DIR / 'train'\n",
    "\n",
    "        #TensorBoard - log model statistics\n",
    "        self.LOGDIR = self.MAIN_DIR / 'my_logs'\n",
    "        \n",
    "        self.MODEL_DIR = self.MAIN_DIR / 'models'\n",
    "        \n",
    "        self.BASE_URL = 'https://raw.githubusercontent.com/hoganj15/Waste_Image_Classifier/main/images_multiclass'\n",
    "        \n",
    "        self.set_up()\n",
    "        \n",
    "        # some tricks to load models by name\n",
    "        from inspect import getmembers, isfunction\n",
    "        self.available_models = {o[0].lower():o[1]for o in getmembers(keras.applications) if isfunction(o[1])}\n",
    "        \n",
    "        # Preprocessing etc. is costly, so train only if no existing model is available\n",
    "        self.model_path = self.MODEL_DIR / model_name\n",
    "        if self.model_path.exists():\n",
    "            print('Model exists, not training.')\n",
    "            self.model = tf.keras.models.load_model(model_path)\n",
    "        \n",
    "        else:\n",
    "            model_instantiator = self.available_models[model_name]\n",
    "            model_module = importlib.import_module('keras.applications' + '.' + model_name)\n",
    "            self.preprocess(model_module.preprocess_input)\n",
    "            self.train(model_instantiator, weights=\"imagenet\", include_top=False, input_shape=(224, 224, 3))\n",
    "        \n",
    "    \n",
    "    def get_run_logdir(self):\n",
    "        run_id = time.strftime('run_%Y_%m_%d-%H_%M_%S')\n",
    "        return self.LOGDIR / run_id\n",
    "\n",
    "    \n",
    "    def set_up(self):\n",
    "        if not self.TEST_PATH.exists():\n",
    "            self.TEST_PATH.mkdir(parents=True)\n",
    "        if not self.TRAIN_PATH.exists():\n",
    "              self.TRAIN_PATH.mkdir(parents=True)\n",
    "        #reading in the files required for image imports\n",
    "\n",
    "        test_csv_path = self.TEST_PATH / 'test_files.csv'       \n",
    "        if not test_csv_path.exists():\n",
    "            r = requests.get(self.BASE_URL + '/' + test_csv_path.name)\n",
    "            test_csv_path.write_text(r.text)\n",
    "        \n",
    "        \n",
    "        train_csv_path = self.TRAIN_PATH / pathlib.Path('train_files.csv')\n",
    "        if not train_csv_path.exists():\n",
    "            r = requests.get(self.BASE_URL + '/' + train_csv_path.name)\n",
    "            train_csv_path.write_text(r.text)\n",
    "\n",
    "        file_names = pd.read_csv(test_csv_path)\n",
    "        for label in file_names.label.unique().tolist():\n",
    "            label_path = self.TEST_PATH / label\n",
    "            if not label_path.exists():\n",
    "                label_path.mkdir() #create a folder for each class in the test directory\n",
    "            files_to_read = file_names[file_names['label'] == label] #filter the files to read for that folder\n",
    "            if len(list(label_path.iterdir())) < len(files_to_read): \n",
    "                for file_name in files_to_read['file_name'].tolist():\n",
    "                    url = self.BASE_URL + '/' + 'test' + '/' + label + '/' + file_name\n",
    "                    r = requests.get(url)\n",
    "                    file_path = label_path / file_name\n",
    "                    file_path.write_bytes(r.content)\n",
    "   \n",
    "        file_names = pd.read_csv(train_csv_path)\n",
    "        for label in file_names.label.unique().tolist():\n",
    "            label_path = self.TRAIN_PATH / label\n",
    "            if not label_path.exists():\n",
    "                label_path.mkdir() #create a folder for each class in the train directory\n",
    "            files_to_read = file_names[file_names['label'] == label] #filter the files to read for that folder\n",
    "            if len(list(label_path.iterdir())) < len(files_to_read): #only read in images if they aren't already there\n",
    "                for file_name in files_to_read['file_name'].tolist():\n",
    "                    url = self.BASE_URL + '/' + 'train' + '/' + label + '/' + file_name\n",
    "                    r = requests.get(url)\n",
    "                    file_path = label_path / file_name\n",
    "                    file_path.write_bytes(r.content)\n",
    "                    \n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "    def preprocess(self, preprocess_function):\n",
    "        original_size = (512, 384)\n",
    "        target_size = (224, 224)\n",
    "        batch_size = 32\n",
    "        process = partial(process_img, crop=self.crop)\n",
    "        \n",
    "        # resize, rescale, random crop and random flip\n",
    "        # Set preprocessing based on the crop switch above\n",
    "        \n",
    "        if self.crop:\n",
    "            preprocessing = tf.keras.Sequential([\n",
    "                keras.layers.RandomFlip(\"horizontal_and_vertical\"),\n",
    "                keras.layers.RandomCrop(*target_size),\n",
    "                keras.layers.Resizing(*target_size),\n",
    "                keras.layers.Rescaling(1./255),\n",
    "            ])\n",
    "        \n",
    "        else:\n",
    "            preprocessing = tf.keras.Sequential([\n",
    "                keras.layers.Resizing(*target_size),\n",
    "                keras.layers.Rescaling(1./255),\n",
    "            ])\n",
    "        \n",
    "        folder_names = ['cardboard', 'glass', 'metal', 'paper', 'plastic', 'trash']\n",
    "        \n",
    "        train_ds = image.image_dataset_from_directory(\n",
    "            '/home/sarp/DS5460/garbage_classifier/train/',\n",
    "            validation_split=0.2,\n",
    "            subset='training',\n",
    "            seed=123,\n",
    "            image_size=original_size,\n",
    "            batch_size=batch_size,\n",
    "        )\n",
    "\n",
    "        val_ds = image.image_dataset_from_directory(\n",
    "            '/home/sarp/DS5460/garbage_classifier/train/',\n",
    "            validation_split=0.2,\n",
    "            subset='validation',\n",
    "            seed=123,\n",
    "            image_size=original_size,\n",
    "            batch_size=batch_size,            \n",
    "        )\n",
    "\n",
    "        test_ds = image.image_dataset_from_directory(\n",
    "            '/home/sarp/DS5460/garbage_classifier/test/',\n",
    "            seed=123,\n",
    "            image_size=original_size,\n",
    "            batch_size=batch_size,\n",
    "        )\n",
    "        \n",
    "        self.train_ds = train_ds.map(lambda x, y: (preprocess_function(preprocessing(x)), y))\n",
    "        self.val_ds = test_ds.map(lambda x, y: (preprocess_function(preprocessing(x)), y))\n",
    "        self.test_ds = test_ds.map(lambda x, y: (preprocess_function(preprocessing(x)), y))\n",
    "\n",
    "        \n",
    "    \n",
    "\n",
    "\n",
    "        \n",
    "        \n",
    "        \n",
    "#         train_generator = image.image_dataset_from_directory(\n",
    "#             rescale=1./255, \n",
    "#             horizontal_flip=True, \n",
    "#             preprocessing_function=process,\n",
    "#         )\n",
    "        \n",
    "#         train_set = train_generator.flow_from_directory(IMAGES_PATH, target_size=(224, 224), subset='training')\n",
    "#         valid_set = train_generator.flow_from_directory(IMAGES_PATH, target_size=(224, 224), subset='validation')\n",
    "        \n",
    "#         for folder in folder_names:\n",
    "#             FOLDER_PATH = self.TEST_PATH / folder\n",
    "#             for im in FOLDER_PATH.iterdir():\n",
    "#                 #load image and transform to array\n",
    "#                 img = image.load_img(str(im))\n",
    "#                 img_array = self.process_img(img, True)\n",
    "#                 processed_img = preprocess_function(img_array)\n",
    "#                 #append processed image to new list\n",
    "#                 X_valid_list.append(list(processed_img))\n",
    "#                 #append label of processed image to new list\n",
    "#                 y_valid_list += [folder]\n",
    "\n",
    "#                 if len(y_valid_list)%50 == 0:\n",
    "#                     print(len(y_valid_list), 'images loaded into test')\n",
    "\n",
    "#         for folder in folder_names:\n",
    "#             FOLDER_PATH = self.TRAIN_PATH / folder\n",
    "#             for im in FOLDER_PATH.iterdir():\n",
    "#                 #load image and transform to array\n",
    "#                 img = image.load_img(str(im))\n",
    "#                 img_array = self.process_img(img, True)\n",
    "#                 processed_img = preprocess_function(img_array)\n",
    "#                 #append processed image to new list \n",
    "#                 X_train_list.append(list(processed_img))\n",
    "#                 #append label of processed image to new list \n",
    "#                 y_train_list += [folder] \n",
    "\n",
    "#                 if len(y_train_list)%50 == 0:\n",
    "#                     print(len(y_train_list), 'images loaded into train')\n",
    "\n",
    "#         X_train_full = np.array(X_train_list).astype(np.float32)\n",
    "#         X_valid = np.array(X_valid_list).astype(np.float32)\n",
    "#         print(X_train_full.shape)\n",
    "#         print(X_valid.shape)\n",
    "\n",
    "#         y_train_full, y_train_keys = pd.factorize(y_train_list)[0].reshape(-1, 1).astype(np.float32), pd.factorize(y_train_list)[1]\n",
    "#         y_valid, y_valid_keys = pd.factorize(y_valid_list)[0].reshape(-1, 1).astype(np.float32), pd.factorize(y_valid_list)[1]\n",
    "\n",
    "#         train_labels = {i: y_train_keys[i] for i in range(len(y_train_keys))}\n",
    "#         valid_labels = {i: y_valid_keys[i] for i in range(len(y_valid_keys))}\n",
    "\n",
    "#         print(y_valid[y_valid_list.index('cardboard')])\n",
    "#         print(y_valid[y_valid_list.index('glass')])\n",
    "#         print(y_valid[y_valid_list.index('metal')])\n",
    "#         print(y_valid[y_valid_list.index('paper')])\n",
    "#         print(y_valid[y_valid_list.index('plastic')])\n",
    "#         print(y_valid[y_valid_list.index('trash')])\n",
    "\n",
    "#         X_valid_list, X_train_list, y_valid_list, y_train_list = [], [], [], []\n",
    "\n",
    "#         valid_shuffler = np.random.permutation(len(X_valid))\n",
    "#         self.X_valid = X_valid[valid_shuffler]\n",
    "#         self.y_valid = y_valid[valid_shuffler]\n",
    "\n",
    "#         train_full_shuffler = np.random.permutation(len(X_train_full))\n",
    "#         X_train_full = X_train_full[train_full_shuffler]\n",
    "#         y_train_full = y_train_full[train_full_shuffler]\n",
    "\n",
    "\n",
    "#         test_size = int(len(y_valid)/2)\n",
    "#         self.X_train, self.X_test = X_train_full[:-test_size], X_train_full[-test_size:]\n",
    "#         self.y_train, self.y_test = y_train_full[:-test_size], y_train_full[-test_size:]\n",
    "        \n",
    "    def train(self, transfer_model, **kwargs):\n",
    "        base_model = transfer_model(kwargs)\n",
    "        \n",
    "        run_logdir = self.get_run_logdir()\n",
    "        tensorboard_callback = keras.callbacks.TensorBoard(run_logdir, update_freq=\"epoch\")\n",
    "\n",
    "        #freeze the base model \n",
    "        base_model.trainable = False\n",
    "\n",
    "        #define the type of NN architecture - sequential model specifies a linear stack of layers \n",
    "        model = keras.models.Sequential()\n",
    "\n",
    "        #add the pre-trained model\n",
    "        model.add(base_model)\n",
    "\n",
    "        #pool layer to prepare data as input into dense layer \n",
    "        #model.add(keras.layers.GlobalAveragePooling2D())\n",
    "        model.add(keras.layers.Dense(256, activation='relu'))\n",
    "        #batch normalization layer re-centers and re-scales the network - helps accelerate training\n",
    "        model.add(keras.layers.BatchNormalization())\n",
    "        #dropout layer - temporarily deactivates 20% of the nodes in the network each epoch to redistribute weights/help network concentrate on \"weak\" features and prevent overfitting\n",
    "        model.add(keras.layers.Dropout(0.2))\n",
    "        #flatten layer to single array for input into dense layer \n",
    "        model.add(keras.layers.Flatten())\n",
    "        # prediction layer - 6 neurons = 6 category outputs, and softmax to normalize the output of the network to a probability distribution over the predicted output classes \n",
    "        model.add(keras.layers.Dense(6, activation='softmax'))\n",
    "        #compile model, specify sparse categorical crossentropy for classification and evaluate accuracy\n",
    "        model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "        #train model for 40 epochs \n",
    "        history = model.fit(self.train_ds, epochs=40, validation_data = self.val_ds, callbacks = [keras.callbacks.EarlyStopping(patience=3, monitor='val_loss'), OverfittingCallback(), tensorboard_callback])\n",
    "        #unfreeze base model \n",
    "        base_model.trainable = True\n",
    "        model.summary()\n",
    "        history = model.fit(self.train_ds, epochs=40, validation_data=self.val_ds, callbacks=[keras.callbacks.EarlyStopping(patience=1, monitor='val_loss'), OverfittingCallback()])\n",
    "        model.save(self.model_path)\n",
    "        \n",
    "    def class_convert(self, classess):\n",
    "        pred=[]\n",
    "        for i in classess:\n",
    "            if i ==0:\n",
    "                pred.append('Cardboard')\n",
    "            elif i==1:\n",
    "                pred.append('Glass')\n",
    "            elif i==2:\n",
    "                pred.append('Metal')\n",
    "            elif i==3:\n",
    "                pred.append('Paper')\n",
    "            elif i==4:\n",
    "                pred.append('Plastic')\n",
    "            elif i==5:\n",
    "                pred.append('Trash')\n",
    "        return pred\n",
    "\n",
    "    def test_model(self):\n",
    "        incorrect = dict()\n",
    "        for idx, predictions in enumerate(self.model.predict(X_test)):\n",
    "            if np.argmax(predictions) != y_test[idx][0]:\n",
    "                incorrect[idx] = {'Predicted': np.argmax(predictions), 'Actual': y_test[idx][0]}\n",
    "\n",
    "        incorrect_preds = pd.DataFrame.from_dict(incorrect, orient='index')\n",
    "        incorrect_preds.reset_index()\n",
    "        #generate predictions using the test dataset\n",
    "        predicted_class=np.argmax(self.model.predict(X_test), axis = -1)\n",
    "        predicted_class\n",
    "        corr=0\n",
    "        false=0\n",
    "        for i in range(len(y_test)):\n",
    "            if y_test[i]==predicted_class[i]:\n",
    "                 corr=corr+1\n",
    "            else:\n",
    "                false=false+1\n",
    "\n",
    "        print(\"Correct:\",corr)\n",
    "        print(\"False\",false)\n",
    "        print(\"accuracy:\", corr / (false+corr))\n",
    "\n",
    "        pred_class=self.class_convert(predicted_class)\n",
    "        y_class=self.class_convert(y_test)\n",
    "\n",
    "        cm=confusion_matrix(y_class,pred_class)\n",
    "        df_cm = pd.DataFrame(cm, range(cm.shape[0]), range(cm.shape[1]))\n",
    "        df_cm.columns = ['Cardboard','Glass','Metal','Paper','Plastic','Trash']\n",
    "        df_cm.index = ['Cardboard','Glass','Metal','Paper','Plastic','Trash']\n",
    "\n",
    "        fig = plt.subplots(figsize=size_)\n",
    "        sb.set(font_scale=1)\n",
    "        sb.heatmap(df_cm, annot=True, annot_kws={\"size\": 10}, cmap=\"viridis\") # font size\n",
    "        plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "weights=\"imagenet\", include_top=False, input_shape=(224, 224, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vLiMSvlEnKDt"
   },
   "source": [
    "#### 4.3: Implement Custom Callback functions \n",
    "\n",
    "Here, we implement two custom callback functions to use when training the model. The first callback creates a custom function which stops training if training accuracy is over 0.999, to prevent overfitting. The second callback logs model statistics which we can view later on to review the training progress."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dSg2bQMhg_i9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2009 files belonging to 6 classes.\n",
      "Using 1608 files for training.\n",
      "Found 2009 files belonging to 6 classes.\n",
      "Using 401 files for validation.\n",
      "Found 518 files belonging to 6 classes.\n",
      "WARNING:tensorflow:Using a while_loop for converting RngReadAndSkip cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting StatelessRandomUniformIntV2 cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting RngReadAndSkip cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting StatelessRandomUniformIntV2 cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting RngReadAndSkip cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting StatelessRandomUniformIntV2 cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting RngReadAndSkip cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting StatelessRandomUniformIntV2 cause there is no registered converter for this op.\n",
      "Epoch 1/40\n",
      "32/51 [=================>............] - ETA: 2:43 - loss: 1.7699 - accuracy: 0.2148"
     ]
    }
   ],
   "source": [
    "classifier = TrashClassifer(model_name='vgg19', crop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "xJopAWmXnuS2"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-804d2270d7306eb4\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-804d2270d7306eb4\");\n",
       "          const url = new URL(\"/\", window.location);\n",
       "          const port = 6006;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%load_ext tensorboard\n",
    "%tensorboard --logdir=./my_logs --port=6006"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7juzYwDSo2N-"
   },
   "source": [
    "## Step 6: Generate Predictions & View Model Results\n",
    "\n",
    "####6.1: View Incorrect Predictions\n",
    "First, let's look at the predictions that were classified incorrectly. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def class_convert(classess):\n",
    "    pred=[]\n",
    "    for i in classess:\n",
    "        if i ==0:\n",
    "            pred.append('Cardboard')\n",
    "        elif i==1:\n",
    "            pred.append('Glass')\n",
    "        elif i==2:\n",
    "            pred.append('Metal')\n",
    "        elif i==3:\n",
    "            pred.append('Paper')\n",
    "        elif i==4:\n",
    "            pred.append('Plastic')\n",
    "        elif i==5:\n",
    "            pred.append('Trash')\n",
    "    return pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "-4DvyB7U8rj2"
   },
   "outputs": [],
   "source": [
    "def test_model(model_path):\n",
    "    model = tf.keras.models.load_model(model_path)\n",
    "    incorrect = dict()\n",
    "    for idx, predictions in enumerate(model.predict(X_test)):\n",
    "      if np.argmax(predictions) != y_test[idx][0]:\n",
    "        incorrect[idx] = {'Predicted': np.argmax(predictions), 'Actual': y_test[idx][0]}\n",
    "\n",
    "    incorrect_preds = pd.DataFrame.from_dict(incorrect, orient='index')\n",
    "    incorrect_preds.reset_index()\n",
    "    #generate predictions using the test dataset\n",
    "    predicted_class=np.argmax(model.predict(X_test), axis = -1)\n",
    "    predicted_class\n",
    "    corr=0\n",
    "    false=0\n",
    "    for i in range(len(y_test)):\n",
    "        if y_test[i]==predicted_class[i]:\n",
    "             corr=corr+1\n",
    "        else:\n",
    "            false=false+1\n",
    "\n",
    "    print(\"Correct:\",corr)\n",
    "    print(\"False\",false)\n",
    "    print(\"accuracy:\", corr / (false+corr))\n",
    "\n",
    "    pred_class=class_convert(predicted_class)\n",
    "    y_class=class_convert(y_test)\n",
    "\n",
    "    cm=confusion_matrix(y_class,pred_class)\n",
    "    df_cm = pd.DataFrame(cm, range(cm.shape[0]), range(cm.shape[1]))\n",
    "    df_cm.columns = ['Cardboard','Glass','Metal','Paper','Plastic','Trash']\n",
    "    df_cm.index = ['Cardboard','Glass','Metal','Paper','Plastic','Trash']\n",
    "\n",
    "    fig = plt.subplots(figsize=size_)\n",
    "    sb.set(font_scale=1)\n",
    "    sb.heatmap(df_cm, annot=True, annot_kws={\"size\": 10}, cmap=\"viridis\") # font size\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "j2R0dJrfpccC"
   },
   "source": [
    "\n",
    "####6.2: Visualize Predictions \n",
    "Since the CNN classifier is only capable of generating numerical predictions from 0-5, we can now generate predictions and then assign them back to their associated garbage class. After doing so, we can plot each image, its true label, and compare this to the predicted label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "xLxLIJOSdv_G"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'plt' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-f5ba60fd8c94>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mL\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m7\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mW\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m7\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mfig\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0maxes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msubplots\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mL\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mW\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mfigsize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m17\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m17\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0maxes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mravel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'plt' is not defined"
     ]
    }
   ],
   "source": [
    "#plot the image from the test dataset, the predicted garbage label, and the actual garbage label\n",
    "\n",
    "L=7\n",
    "W=7\n",
    "fig,axes=plt.subplots(L,W,figsize=(17,17))\n",
    "axes=axes.ravel()\n",
    "\n",
    "for i in np.arange(0,L*W):\n",
    "    axes[i].imshow(X_test[i])\n",
    "    if pred_class[i] == y_class[i]:\n",
    "      axes[i].set_title('Prediction={}\\n True={}'.format(pred_class[i],y_class[i]), color='C2')\n",
    "    else: \n",
    "      axes[i].set_title('Prediction={}\\n True={}'.format(pred_class[i],y_class[i]), color='C3')\n",
    "    axes[i].axis('off')\n",
    "\n",
    "plt.subplots_adjust(wspace=2,hspace=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
